{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1758084828896,
     "user": {
      "displayName": "Solomon Freer",
      "userId": "07757358301173662182"
     },
     "user_tz": -600
    },
    "id": "oiE36baUEw1h"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PV Lighthouse Uncertainty Tool\n",
    "Helper file\n",
    "Contains many functions used for connecting to the uncertainty API and plotting and analysing its output\n",
    "\"\"\"\n",
    "\n",
    "# Define constants for plotting\n",
    "FIGURE_FONT_SIZE = 16           # Font size for figure axes, titles will be (FIGURE_FONT_SIZE + 2).\n",
    "TRANSPARENT_PLOTS = False        # False = white background, True = transparent (best for adding into presentations)\n",
    "HISTOGRAM_COLOUR = \"#d2226a\"    # Hex code for the colour used in column plots\n",
    "GRID_COLOUR = \"#c0c0c0\"       # Colour of gridlines in plots\n",
    "GRID_WIDTH = 0.1                # Thickness of gridlines in plots\n",
    "\n",
    "# Import P90Client and related tools\n",
    "from pvl_p90_client.client.p90_client import AuthenticationError, ServiceError, P90Client, P90ConnectionError\n",
    "from pvl_p90_client.grpcclient import uncertaintyMessages_pb2, weatherData_pb2\n",
    "from pvl_p90_client.grpcclient.uncertaintyMessages_pb2 import DistributionInput, DistributionFunction\n",
    "from pvl_p90_client.helpers import pvl_login\n",
    "from pvl_p90_client.helpers.exception import TimeStepDataError\n",
    "from pvl_p90_client.helpers.request_helpers import (\n",
    "    build_request,\n",
    "    build_gaussian_distribution,\n",
    "    build_skewed_gaussian_distribution,\n",
    "    build_weibull_distribution,\n",
    "    build_arbitrary_distribution,\n",
    "    build_distribution,\n",
    "    build_module_info,\n",
    "    build_result_options,\n",
    "    build_simulation_options,\n",
    "    build_system_info,\n",
    "    build_electrical_settings,\n",
    "    build_optical_settings,\n",
    "    build_thermal_settings,\n",
    "    build_operational_settings,\n",
    "    load_weather_data_from_pvw_file,\n",
    "    load_time_step_data_from_csv,\n",
    "    write_timestep_data_to_csv,\n",
    "    celsius_to_kelvin,\n",
    "    kelvin_to_celsius\n",
    ")\n",
    "\n",
    "\n",
    "# Import other Python libraries\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, skewnorm\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, Latex, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "try:\n",
    "    # Enable the custom widget manager for Colab for interactive plots\n",
    "    from google.colab import output\n",
    "    # pio.renderers.default = 'colab'\n",
    "    output.enable_custom_widget_manager()       # Needed to make plotly interactive plots work correctly in Colab\n",
    "except:\n",
    "    # Not using Google Colab\n",
    "    None\n",
    "try:\n",
    "    from openpyxl.workbook import Workbook\n",
    "except:\n",
    "    # Install openpyxl\n",
    "    %pip install openpyxl\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Define dictionary of limits for each DistributionInput type\n",
    "DISTRIBUTION_LIMITS =  {\n",
    "    DistributionInput.GHI: (0, float('inf')),                               # Global Horizontal Irradiance (W/m²)\n",
    "    DistributionInput.DiffuseFraction: (0, float('inf')),\n",
    "    DistributionInput.WindSpeed: (0, float('inf')),                         # Wind speed (m/s)\n",
    "    DistributionInput.Temperature: (0, float('inf')),                       # Ambient temperature\n",
    "    DistributionInput.ModulePower: (0, 1),\n",
    "    DistributionInput.SpectralCorrection: (0, float('inf')),                # Spectral correction factor\n",
    "    DistributionInput.SoilingFront: (0, 1),                                 # Front soiling fraction\n",
    "    DistributionInput.SoilingRear: (0, 1),                                  # Rear soiling fraction\n",
    "    DistributionInput.Uc: (0, float('inf')),                                # Thermal coefficient Uc (W/m²K)\n",
    "    DistributionInput.Uv: (0, float('inf')),                                # Thermal coefficient Uv (W/m³Ks)\n",
    "    DistributionInput.Alpha: (0, float('inf')),                             # Thermal absorptance of module\n",
    "    DistributionInput.AnnualDegradationRate: (0, 1),\n",
    "    DistributionInput.Availability: (0, 1),                                 # System Availability\n",
    "    DistributionInput.YieldModifier: (0, float('inf')),                     # Yield modifier\n",
    "    DistributionInput.CircumsolarFraction: (0, float('inf')),               # Circumsolar fraction of incident light\n",
    "    DistributionInput.UndulatingGround: (0, float('inf')),                  # Undulating ground modifier\n",
    "    DistributionInput.ExtraIrradiance: (0, float('inf')),                   # Extra irradiance modifier\n",
    "    DistributionInput.Curtailment: (0, 1),                                  # System curtailment\n",
    "    DistributionInput.DCHealth: (0, 1),\n",
    "    DistributionInput.InverterToInverterMismatch: (0, 1),\n",
    "    DistributionInput.StringToStringMismatch: (0, 1),\n",
    "    DistributionInput.ModuleToModuleMismatch: (0, 1),\n",
    "    DistributionInput.CellToCellMismatch: (0, 1),\n",
    "    DistributionInput.InverterEfficiency: (0, 1),\n",
    "    DistributionInput.ModuleEfficiencyTemperatureCoefficient: (0, float('inf')), # Module efficiency temperature coefficient (/K)\n",
    "    DistributionInput.Albedo: (0, 1),\n",
    "    DistributionInput.RearTransmissionFactor: (0, float('inf')),          # Rear transmission factor (f_T, 'Shed transparent fraction')\n",
    "    DistributionInput.RearStructuralShadingFactor: (0, 1)                 # Rear structural shading factor (f_S)\n",
    "}\n",
    "\n",
    "#### Define helper functions ####\n",
    "# Define function to load weather data from either a .pvw file or a CSV file\n",
    "def load_weather_data(file_path: str) -> list[uncertaintyMessages_pb2.TimeStepData] | None:\n",
    "    \"\"\"Load weather data from a .pvw file or a CSV file.\"\"\"\n",
    "    if file_path.lower().endswith('.pvw'):\n",
    "        weather_data = load_weather_data_from_pvw_file(file_path)\n",
    "    elif file_path.lower().endswith('.csv'):\n",
    "        weather_data = load_time_step_data_from_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .pvw or .csv file.\")\n",
    "    return weather_data\n",
    "\n",
    "# Define weibull_cdf function for checking limits are compatible with inversion\n",
    "def weibull_cdf(x, x0, lambda_term, k, p):\n",
    "                if p * (x - x0) > 0:\n",
    "                    return 0.5 + p * (0.5 - math.exp(-1 * (p * (x - x0) / lambda_term) ** k))\n",
    "                else:\n",
    "                    return 0 if p > 0 else 1\n",
    "\n",
    "def create_distribution(input, simToSim=None, yearToYear=None, stepToStep=None) -> uncertaintyMessages_pb2.Distribution:\n",
    "    \"\"\"Creates an input distribution with user-input values.\n",
    "        input: the integer corresponding to the system variable which is being modified (0-16)\n",
    "            GHI: 0, DiffuseFraction: 1, WindSpeed: 2, Temperature: 3, ModulePower: 4, SpectralCorrection: 5, SoilingFront: 6, SoilingRear: 7,\n",
    "            Uc: 8, Uv: 9, Alpha: 10, AnnualDegradationRate: 11, Availability: 12, YieldModifier: 13, CircumsolarFraction: 14, UndulatingGround: 15,\n",
    "            ExtraIrradiance: 16, Curtailment: 17, DCHealth: 18, InverterToInverterMismatch: 19, StringToStringMismatch: 20, ModuleToModuleMismatch: 21,\n",
    "            CellToCellMismatch: 22, InverterEfficiency: 23, ModuleEfficiencyTemperatureCoefficient: 24, Albedo: 25, RearStructuralShadingFactor: 26, RearTransmissionFactor: 27\n",
    "        {X}_pdf: the probability distribution function (pdf) for {X} uncertainty (simulation, yearly, or time_step)\n",
    "                    pdf format: ['FunctionName', *parameters], pdfs: 'Gaussian', 'SkewedGaussian', 'Weibull', or 'Arbitrary'\n",
    "        simToSim: the pdf for simulation error (e.g. ['Gaussian', 0.01, 0.1] ; x0 = 0.01, σ = 0.1)\n",
    "        yearToYear: the pdf for yearly error (e.g. ['Weibull', 0.0, 0.025, 1.7] ; x0 = 0, λ = 0.025, k = 1.7)\n",
    "        time_pdf: the pdf for time step error (e.g. ['SkewedGaussian', 1.0, 3.2, -2.0] ; ɑ = -2, ζ = 1, ⍵ = 3.2)\n",
    "        If any pdf's are None then they are not used.\n",
    "    \"\"\"\n",
    "    if not input in range(27):\n",
    "        raise ValueError(f\"Invalid simulation input: {input}\")\n",
    "    \n",
    "    # Check if any distributions are Weibull and ensure that the limits of the input don't give the same CDF value (to avoid error)\n",
    "    distributions_to_check = [dist for dist in [simToSim, yearToYear, stepToStep] if dist is not None]\n",
    "    weibull_distributions = [dist for dist in distributions_to_check if dist[0] == \"Weibull\"]\n",
    "    if weibull_distributions:\n",
    "        lower_limit, upper_limit = DISTRIBUTION_LIMITS[input]\n",
    "        for weibull_dist in weibull_distributions:\n",
    "            # Extract Weibull parameters: x0, lambda_term, k\n",
    "            x0, lambda_term, k = weibull_dist[1:4]\n",
    "            positive_polarity = weibull_dist[4] if len(weibull_dist) > 4 else True  # Default is True, if unprovided\n",
    "            p = 1 if positive_polarity else -1\n",
    "            # Check if the CDF values at the limits would be the same\n",
    "            cdf_lower = weibull_cdf(lower_limit, x0, lambda_term, k, p)\n",
    "            cdf_upper = weibull_cdf(upper_limit, x0, lambda_term, k, p)\n",
    "\n",
    "            if abs(cdf_upper - cdf_lower) < 1e-8:\n",
    "                raise ValueError(f\"\"\"Weibull distribution parameters for '{DistributionInput.Name(input)}' result in identical CDF values at limits:\n",
    "    [{lower_limit}, {upper_limit}]->[{cdf_lower}, {cdf_upper}].\n",
    "Adjust your Weibull distribution to have a variation between [{lower_limit}, {upper_limit}], probably by changing the x0 or polarity values.\"\"\")\n",
    "\n",
    "    input_distribution = build_distribution(\n",
    "        input=input,\n",
    "        sim_to_sim_distribution=simToSim and build_pdf_from_params(simToSim[0], *simToSim[1:]),   # 'and' ensures {X}_pdf=None is passed as None\n",
    "        yr_to_yr_distribution=yearToYear and build_pdf_from_params(yearToYear[0], *yearToYear[1:]),\n",
    "        step_to_step_distribution=stepToStep and build_pdf_from_params(stepToStep[0], *stepToStep[1:])\n",
    "    )\n",
    "    return input_distribution\n",
    "\n",
    "def build_constant_distribution(value: float) -> DistributionFunction:\n",
    "    \"\"\"Builds a constant distribution.\"\"\"\n",
    "    constant_dist = DistributionFunction()\n",
    "    constant_dist.Type = DistributionType.Constant\n",
    "    constant_dist.Constant.Value = value\n",
    "    return constant_dist\n",
    "\n",
    "def build_pdf_from_params(pdf_name, *params):\n",
    "    # Simple method of generically sending pdf build requests\n",
    "    if pdf_name == \"Gaussian\":\n",
    "        # build_gaussian_distribution( x0: float, sigma: float, fmax: float | None = None, num_points_in_probability_function: int | None = None, lower_limit: float | None = None, upper_limit: float | None = None, )\n",
    "        return build_gaussian_distribution(*params)\n",
    "    elif pdf_name == \"SkewedGaussian\":\n",
    "        # build_skewed_gaussian_distribution( alpha: float, zeta: float, omega: float, fmax: float | None = None, num_points_in_probability_function: int | None = None, )\n",
    "        return build_skewed_gaussian_distribution(params[-1], *params[0:-1])   # TODO: change to (*params) once backend reorders arguments so skewness is last\n",
    "    elif pdf_name == \"Weibull\":\n",
    "        # build_weibull_distribution(x0: float, lambda_term: float, k: float, has_positive_polarity: bool | None = None)\n",
    "        return build_weibull_distribution(*params)\n",
    "    elif pdf_name == \"Arbitrary\":\n",
    "        # build_arbitrary_distribution(x: list[float], y: list[float])\n",
    "        return build_arbitrary_distribution(*params)\n",
    "    elif pdf_name == \"Constant\":\n",
    "        # build_constant_distribution(value: float)\n",
    "        return build_constant_distribution(*params)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid pdf name: {pdf_name}\")\n",
    "\n",
    "def perform_analysis(uncertainty_client, uncertainty_request, call_credentials, show_errors=True):\n",
    "    \"\"\"Perform the uncertainty analysis and display results.\"\"\"\n",
    "    try:\n",
    "        print(\"Starting uncertainty analysis...\")\n",
    "        t_start = time.time()    # get the start time\n",
    "        summary: UncertaintySummary | None = uncertainty_client.send_request( #uncertainty_client,\n",
    "            uncertainty_request, call_credentials, timeout=60*14\n",
    "        )  # timeout in seconds\n",
    "        t_request = time.time()-t_start    # get the request time in seconds\n",
    "        n_total = uncertainty_request.SimulationOptions.NumberOfSimulations*uncertainty_request.SimulationOptions.NumberOfYears\n",
    "        print(f\"Request took {round(t_request)} s, running {n_total} simulations in total ({round(n_total/t_request)} sims/s)\")\n",
    "        # Display results\n",
    "        if summary:\n",
    "            return summary\n",
    "        else:\n",
    "            print(\"⚠️  No results received from analysis\")\n",
    "    except AuthenticationError as e:\n",
    "        if show_errors:\n",
    "            print(f\"❌ Authentication Error: {e}\")\n",
    "    except P90ConnectionError as e:\n",
    "        if show_errors:\n",
    "            print(f\"❌ Connection Error: {e}\")\n",
    "    except ServiceError as e:\n",
    "        if show_errors:\n",
    "            print(f\"❌ Service Error: {e}\")\n",
    "    except TimeoutError as e:\n",
    "        if show_errors:\n",
    "            print(f\"❌ Timeout Error: {e}\")\n",
    "    except ValueError as e:\n",
    "        if show_errors:\n",
    "            print(f\"❌ Invalid Request: {e}\")\n",
    "    except (RuntimeError, ConnectionError) as e:\n",
    "        if show_errors:\n",
    "            print(f\"❌ Unexpected Error: {e}\")\n",
    "\n",
    "def request_analysis(p90_request):\n",
    "    with P90Client(version=\"latest-dev\") as client:\n",
    "        # User will be prompted to enter credentials\n",
    "        credentials = pvl_login.login()\n",
    "        # Send request and get results\n",
    "        summary = perform_analysis(client, p90_request, credentials)\n",
    "        return summary\n",
    "\n",
    "# Converts the summary data into three pandas DataFrames for easier analysis\n",
    "def summary_to_dataframes(summary_data, inputs_data):\n",
    "    bin_width = inputs_data.ResultOptions.PDelta\n",
    "    # yearly_hist_df: years x bin frequencies\n",
    "    n_bins = len(summary_data.YearlyHistogram[0].bins)\n",
    "    bin_min = 1 - n_bins * bin_width / 2\n",
    "    bin_edges = [float(round(x, 5)) for x in np.linspace(bin_min, bin_min+bin_width*(n_bins-1), n_bins)]\n",
    "    yearly_hist_df = pd.DataFrame([(hist.Year, *hist.bins) for hist in summary_data.YearlyHistogram], \n",
    "                columns=['Year', *bin_edges]).set_index('Year')\n",
    "    \n",
    "    \n",
    "    # yearly_p_df: years x P-values\n",
    "    yearly_p_df = pd.DataFrame([(pval.Year, f'P{pval.P}', pval.P50Deviation) for pval in summary_data.YearlyPValue],\n",
    "                columns=['Year', 'P-value', 'Value']).pivot(index='Year', columns='P-value', values='Value')\n",
    "\n",
    "    # Sort columns by P-value in descending order (e.g. P95, P90, P10, P5)\n",
    "    yearly_p_df = yearly_p_df.reindex(sorted(yearly_p_df.columns, key=lambda x: int(x[1:]), reverse=True), axis=1)\n",
    "\n",
    "    # yearly_p50_df: simple year and P50 deviation\n",
    "    yearly_p50_df = pd.DataFrame([(p50.Year, p50.Value) for p50 in summary_data.YearlyP50Deviation], \n",
    "                columns=['Year', 'P50_Deviation']).set_index('Year')\n",
    "    \n",
    "    return yearly_hist_df, yearly_p_df, yearly_p50_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Rm69NSpOvp-n"
   },
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "\n",
    "# Apply default styles to a plotly figure (Note: FIGURE_FONT_SIZE and TRANSPARENT_PLOTS are defined at the top of this notebook)\n",
    "def apply_styles(fig, image=True):\n",
    "    fig.update_layout(\n",
    "        font=dict(size=FIGURE_FONT_SIZE),\n",
    "        title_font=dict(size=FIGURE_FONT_SIZE+2),\n",
    "        paper_bgcolor = \"rgba(0,0,0,0)\" if TRANSPARENT_PLOTS else \"white\",  # Fully transparent background\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\" if TRANSPARENT_PLOTS else \"white\"\n",
    "    )\n",
    "    if image:\n",
    "        # Load the SunSolve Logo and convert to base64\n",
    "        image_path = \"Images/SunSolveLogo.svg\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_image = base64.b64encode(image_file.read()).decode()\n",
    "        fig.update_layout(images=[dict(\n",
    "            source=f\"data:image/svg+xml;base64,{encoded_image}\",    # Add SunSolve Logo to plot\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.4, y=1.18,\n",
    "            sizex=0.2, sizey=0.2,\n",
    "            xanchor=\"left\", yanchor=\"top\"\n",
    "        )])\n",
    "    fig.update_xaxes(linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
    "    fig.update_yaxes(linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\", showgrid=True, gridwidth=GRID_WIDTH, gridcolor=GRID_COLOUR)\n",
    "\n",
    "def plot_yearly_hist_plotly(data, year_index, y_limit=None, fig_width=900, bin_min=0.75, bin_width=0.01,\n",
    "                            yearly_yield=1, yield_units=None, x_range=None, title=None):\n",
    "    fig = go.Figure()\n",
    "    bin_max = 2-bin_min-bin_width\n",
    "    n_bins = len(data.YearlyHistogram[0].bins)\n",
    "    x_axis = np.linspace(bin_min, bin_max, n_bins) * yearly_yield\n",
    "    tot_freq = sum(data.YearlyHistogram[year_index].bins)\n",
    "    probs = [freq_i/tot_freq*100 for freq_i in data.YearlyHistogram[year_index].bins]   # probabilities are normalised frequencies\n",
    "    prob_limit = math.ceil(y_limit/tot_freq*100+0.5) if y_limit is not None else None\n",
    "\n",
    "    fig.add_trace(go.Bar(x=x_axis, y=probs, marker_color=HISTOGRAM_COLOUR))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Year {data.YearlyHistogram[year_index].Year} Histogram {'- '+title if title else ''}\",\n",
    "        xaxis_title=\"Relative yield\" if yield_units is None else f\"Yield ({yield_units})\",\n",
    "        yaxis_title=\"Probability (%)\",\n",
    "        yaxis_range=[0, prob_limit] if y_limit is not None else None,\n",
    "        width=fig_width,\n",
    "    )\n",
    "    apply_styles(fig)\n",
    "    fig.update_xaxes(minor_ticks=\"outside\")\n",
    "    if x_range is not None:\n",
    "        fig.update_xaxes(range=x_range)     # Set a fixed x-axis range if provided, otherwise auto-scale\n",
    "    fig.show()\n",
    "\n",
    "def plot_interactive_histogram(summary_data, inputs_data, year_one_yield=1, yield_units=\"MWh\", each_year_independent=False, title=None):\n",
    "    bin_min = inputs_data.ResultOptions.PMin\n",
    "    bin_width = inputs_data.ResultOptions.PDelta\n",
    "    max_freq = max(max(summary_data.YearlyHistogram[i].bins) for i in range(len(summary_data.YearlyHistogram)))\n",
    "    year_slider = IntSlider(min=1, max=len(summary_data.YearlyHistogram), step=1, description=\"Year:\")\n",
    "    if year_one_yield == 1:\n",
    "        yield_units = None      # Don't show units if yields are relative to year one\n",
    "\n",
    "    def update_plot(year_index):\n",
    "        yearly_yield = summary_data.YearlyP50Deviation[year_index-1].Value * year_one_yield\n",
    "        xmin = min(dev.Value for dev in summary_data.YearlyP50Deviation) * year_one_yield * bin_min        # Define minimum x-value across all years, to keep plots on same scale as you change year\n",
    "        xmax = max(dev.Value for dev in summary_data.YearlyP50Deviation) * year_one_yield * (2-bin_min)    # Define maximum x-value across all years, as above\n",
    "        x_range = [xmin, xmax]\n",
    "        if each_year_independent:\n",
    "            # Override to make all histograms relative to their own year's P50\n",
    "            x_range = None\n",
    "            yearly_yield = 1\n",
    "        plot_yearly_hist_plotly(summary_data, year_index-1, y_limit=math.ceil(max_freq/10+0.1)*10, bin_min=bin_min, bin_width=bin_width,\n",
    "                                yearly_yield=yearly_yield, yield_units=yield_units, x_range=x_range, title=title)\n",
    "\n",
    "    interactive_plot = interactive(update_plot, year_index=year_slider)\n",
    "    display(interactive_plot)\n",
    "\n",
    "def plot_yearly_P50_Values(data, fig_width=900, year_one_yield=1, yield_units=\"MWh\"):\n",
    "    years, P50Deviations = zip(*[[i.Year, i.Value*year_one_yield] for i in data.YearlyP50Deviation])\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=list(years), y=list(P50Deviations), marker_color=HISTOGRAM_COLOUR))\n",
    "\n",
    "    y_range = math.ceil(1000*(max(P50Deviations)-min(P50Deviations)))/1000\n",
    "    y0 = round(1000*(max(P50Deviations)+min(P50Deviations))/2)/1000\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"P50 Yield Relative to Year 1\" if year_one_yield == 1 else \"P50 Yield by Year\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"Relative P50 yield\" if year_one_yield == 1 else f\"P50 yield ({yield_units})\",\n",
    "        yaxis_range=[y0 - y_range, y0 + y_range],\n",
    "        width=fig_width,\n",
    "    )\n",
    "    apply_styles(fig)\n",
    "    fig.update_xaxes(dtick=1, minor_ticks=None)\n",
    "    fig.show()\n",
    "\n",
    "def plot_pvalues(data, year_one_yield=1, yield_units=\"MWh\"):\n",
    "    # Get unique P values excluding 50\n",
    "    unique_p_values = sorted(list(set([item.P for item in data.YearlyPValue if item.P != 50])))\n",
    "\n",
    "    # Separate P values into < 50 and > 50\n",
    "    p_values_less_than_50 = [p for p in unique_p_values if p < 50]\n",
    "    p_values_greater_than_50 = [p for p in unique_p_values if p > 50]\n",
    "\n",
    "    # Collect the P50 yields relative to year one, for rescaling the deviations\n",
    "    p50_yields = {}\n",
    "    for i in data.YearlyP50Deviation:\n",
    "        p50_yields[i.Year] = i.Value * year_one_yield\n",
    "\n",
    "    # Create subplots and add traces for P-values < 50\n",
    "    if p_values_less_than_50 != []:\n",
    "        fig = make_subplots(rows=1, cols=2)\n",
    "        # Add traces for P-values < 50\n",
    "        for p_value in p_values_less_than_50:\n",
    "            filtered_data = sorted([item for item in data.YearlyPValue if item.P == p_value], key=lambda x: x.Year)\n",
    "            years = [item.Year for item in filtered_data]\n",
    "            p50_deviations = [item.P50Deviation * p50_yields[item.Year] for item in filtered_data]\n",
    "            fig.add_trace(go.Scatter(x=years, y=p50_deviations, mode=\"lines+markers\", name=f\"P{p_value}\"), row=1, col=1)\n",
    "\n",
    "    else:\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "    # Add traces for P-values > 50\n",
    "    for p_value in p_values_greater_than_50:\n",
    "        filtered_data = sorted([item for item in data.YearlyPValue if item.P == p_value], key=lambda x: x.Year)\n",
    "        years = [item.Year for item in filtered_data]\n",
    "        p50_deviations = [item.P50Deviation * p50_yields[item.Year] for item in filtered_data]\n",
    "\n",
    "        if p_values_less_than_50 != []:\n",
    "            fig.add_trace(go.Scatter(x=years, y=p50_deviations, mode=\"lines+markers\", name=f\"P{p_value}\"), row=1, col=2)\n",
    "        else:\n",
    "            fig.add_trace(go.Scatter(x=years, y=p50_deviations, mode=\"lines+markers\", name=f\"P{p_value}\"), row=1, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"P-values{' < 50 and > 50,' if p_values_less_than_50 != [] else ''} {'Relative to Year 1 P50 Yield' if year_one_yield == 1 else ''}\",\n",
    "        width=900,\n",
    "        xaxis={\"type\": \"category\"},\n",
    "        xaxis2={\"type\": \"category\"},\n",
    "        showlegend=True,\n",
    "    )\n",
    "    apply_styles(fig)\n",
    "    fig.update_xaxes(title_text=\"Year\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Relative PXX yield\" if year_one_yield == 1 else f\"PXX yield ({yield_units})\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Year\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Define interactive weather plot function\n",
    "def interactive_weather_plot(weather_path, weather_data):\n",
    "    # Get timezone\n",
    "    if weather_path.lower().endswith(\".pvw\"):\n",
    "        with open(weather_path, \"rb\") as f:\n",
    "            weather_file_text = f.read()\n",
    "            weather_setting = weatherData_pb2.WeatherSetting()\n",
    "            weather_setting.ParseFromString(weather_file_text)\n",
    "        timezone = datetime.timezone(datetime.timedelta(hours=weather_setting.LocalStandardTimeOffset_Minutes.data/60))\n",
    "    else:\n",
    "        timezone = datetime.timezone(datetime.timedelta(hours=0))       # Don't adjust timezone for csv, etc.\n",
    "\n",
    "    # Extract date and time information and irradiance data\n",
    "    dates, hours, ghis, dhis, dnis = [], [], [], [], []\n",
    "\n",
    "    for data_point in weather_data:\n",
    "        # Convert EndOfPeriodUTC to datetime\n",
    "        timestamp_seconds = data_point.EndOfPeriodUTC.seconds\n",
    "        dt_object = datetime.datetime.fromtimestamp(timestamp_seconds, tz=timezone)\n",
    "\n",
    "        dates.append(dt_object.date())\n",
    "        hours.append(dt_object.hour)\n",
    "\n",
    "        # Extract irradiance values, handle cases where they might be missing\n",
    "        ghis.append(getattr(data_point.Weather, \"GHI\", 0))\n",
    "        dhis.append(getattr(data_point.Weather, \"DHI\", 0))\n",
    "        dnis.append(getattr(data_point.Weather, \"DNI\", 0))\n",
    "\n",
    "    # Prepare max_irradiance variable for keeping plot limits consistent across all days\n",
    "    max_irradiance = math.ceil(max(ghis + dhis + dnis)/10)*10\n",
    "\n",
    "    # Create a DataFrame for easier filtering\n",
    "    weather_df = pd.DataFrame({\"Date\": dates, \"Hour\": hours, \"GHI\": ghis, \"DHI\": dhis, \"DNI\": dnis})\n",
    "\n",
    "    # Get unique dates\n",
    "    unique_dates = sorted(weather_df[\"Date\"].unique())\n",
    "\n",
    "    # Set up interactive plot\n",
    "    doy_slider = IntSlider(min=1, max=len(unique_dates), step=1, description=\"Day of year:\")\n",
    "\n",
    "    def update_plot(day_index):\n",
    "        selected_date = unique_dates[day_index-1]\n",
    "\n",
    "        # Filter weather_data for the target date and hours 4-20\n",
    "        daily_data = weather_df[(weather_df[\"Date\"] == selected_date) & (weather_df[\"Hour\"] >= 4) & (weather_df[\"Hour\"] <= 20)]\n",
    "\n",
    "        # Extract data for plotting\n",
    "        hours = daily_data[\"Hour\"]\n",
    "\n",
    "        # Create the plotly plot\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=hours, y=daily_data[\"GHI\"], mode=\"lines+markers\", name=\"GHI\", marker_color=\"#d2226a\", marker_size=4))\n",
    "        fig.add_trace(go.Scatter(x=hours, y=daily_data[\"DHI\"], mode=\"lines+markers\", name=\"DHI\", marker_color=\"#27cfff\", marker_size=4))\n",
    "        fig.add_trace(go.Scatter(x=hours, y=daily_data[\"DNI\"], mode=\"lines+markers\", name=\"DNI\", marker_color=\"#febf24\", marker_size=4))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"Irradiance on {selected_date}\",\n",
    "            xaxis_title=\"Hour of Day\",\n",
    "            yaxis_title=\"Irradiance (W/m²)\",\n",
    "            xaxis_range=[4, 20],\n",
    "            yaxis_range=[0, max_irradiance],\n",
    "            title_x=0.0,\n",
    "            height=300,\n",
    "            width=500,\n",
    "            margin=dict(l=20, r=20, t=35, b=20),\n",
    "        )\n",
    "        apply_styles(fig, image=False)\n",
    "        fig.update_xaxes(dtick=2)\n",
    "        fig.show()\n",
    "\n",
    "    interactive_plot = interactive(update_plot, day_index=doy_slider)\n",
    "    display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PisOe5nRRanI"
   },
   "outputs": [],
   "source": [
    "# Define distribution functions for plotting interactively\n",
    "def gaussian(x, x0, std_dev):\n",
    "    \"\"\"Calculates the probability density function of a Gaussian distribution.\"\"\"\n",
    "    # Ensure x is a numpy array for correct operation with scipy.stats\n",
    "    x = np.asarray(x)\n",
    "    return norm.pdf(x, loc=x0, scale=std_dev)\n",
    "\n",
    "def weibull(x, x0, lamb, k, positive_polarity=True):\n",
    "    \"\"\"Calculates the probability density function of a Weibull distribution.\"\"\"\n",
    "    p = 1 if positive_polarity else -1\n",
    "    # Set to 0 if x is on the wrong side of x0 or exactly equal to x0 (to avoid division by zero)\n",
    "    return 0 if p*(x-x0)<0 or x == x0 else (k/lamb)*(p*(x-x0)/lamb)**(k-1)*math.exp(-1*(p*(x-x0)/lamb)**k)\n",
    "\n",
    "def skewed_gaussian(x, x0, std_dev, skewness):\n",
    "    \"\"\"Calculates the probability density function of a Skewed Gaussian distribution.\"\"\"\n",
    "    # ζ = location (x0), ω = scale (std_dev), α = skewness\n",
    "    x = np.asarray(x)    # Ensure x is a numpy array for correct operation with scipy.stats\n",
    "    return skewnorm.pdf(x, skewness, loc=x0, scale=std_dev)\n",
    "\n",
    "def constant(x, value):\n",
    "    \"\"\"Represents a Constant distribution.\"\"\"\n",
    "    return value\n",
    "\n",
    "# Define parameters for each distribution type\n",
    "distribution_params = {\n",
    "    \"Gaussian\": [\"x0\", \"std_dev\"],\n",
    "    \"Skewed Gaussian\": [\"zeta\", \"omega\", \"alpha\"],\n",
    "    \"Constant\": [\"value\"],\n",
    "    \"Weibull\": [\"x0\", \"k\", \"lamb\", \"polarity\"]\n",
    "}\n",
    "\n",
    "# Define interactive distribution plotter function, to visualise possible modifiers\n",
    "def interactive_distribution_plot():\n",
    "    # Create specific input widgets for each distribution type (initially hidden), also setting default/starting values\n",
    "    std_dev_input = widgets.FloatText(value=0.05, description=\"σ:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.01)\n",
    "    x0_input = widgets.FloatText(value=0.9, description=\"x0:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.01)\n",
    "    k_input = widgets.FloatText(value=2.0, description=\"k:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.1)\n",
    "    lamb_input = widgets.FloatText(value=0.05, description=\"λ:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.01)\n",
    "    polarity_input = widgets.Dropdown(options=[('-1', False), ('+1', True)], value=True, description=\"polarity:\", layout=widgets.Layout(width=\"70%\", display=\"none\"))\n",
    "    zeta_input = widgets.FloatText(value=0.9, description=\"ξ:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.01)\n",
    "    omega_input = widgets.FloatText(value=0.05, description=\"ω:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.01)\n",
    "    alpha_input = widgets.FloatText(value=2.0, description=\"α:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.1)\n",
    "    height_input = widgets.FloatText(value=1.0, description=\"height:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.1)\n",
    "    value_input = widgets.FloatText(value=1.0, description=\"value:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.1)\n",
    "    x1_input = widgets.FloatText(value=1.2, description=\"x1:\", layout=widgets.Layout(width=\"70%\", display=\"none\"), step=0.1)\n",
    "\n",
    "    # Store all parameter widgets in a dictionary for easy access\n",
    "    param_widgets = {\n",
    "        \"std_dev\": std_dev_input,\n",
    "        \"x0\": x0_input,\n",
    "        \"x1\": x1_input,\n",
    "        \"k\": k_input,\n",
    "        \"lamb\": lamb_input,\n",
    "        \"polarity\": polarity_input,\n",
    "        \"zeta\": zeta_input,\n",
    "        \"omega\": omega_input,\n",
    "        \"alpha\": alpha_input,\n",
    "        \"height\": height_input,\n",
    "        \"value\": value_input,\n",
    "        \"x_min\": None, # These will be handled by the main xmin_input and xmax_input\n",
    "        \"x_max\": None\n",
    "    }\n",
    "\n",
    "    # Create main control widgets\n",
    "    distribution_dropdown = widgets.Dropdown(\n",
    "        # options=[\"Gaussian\", \"Skewed Gaussian\", \"Weibull\", \"Delta\", \"Tophat\", \"Constant\"],\n",
    "        options=[\"Gaussian\", \"Skewed Gaussian\", \"Weibull\"],\n",
    "        value=\"Gaussian\",\n",
    "        description=\"Distribution:\",\n",
    "        layout=widgets.Layout(width=\"30%\")\n",
    "    )\n",
    "\n",
    "    plot_header = widgets.HTML(\n",
    "        value=\"<b>Plot controls:</b>\",\n",
    "        description=\"\",\n",
    "    )\n",
    "    xmin_input = widgets.FloatText(value=0.0, description=\"x_min:\", layout=widgets.Layout(width=\"17%\"), step=0.1)\n",
    "    xmax_input = widgets.FloatText(value=2.0, description=\"x_max:\", layout=widgets.Layout(width=\"17%\"), step=0.1)\n",
    "    n_input = widgets.IntText(value=200, description=\"n_points:\", layout=widgets.Layout(width=\"17%\"), step=50)\n",
    "\n",
    "    # Create an Output widget to specifically hold the plot output\n",
    "    plot_output = widgets.Output()\n",
    "\n",
    "    def update_param_visibility(distribution_type):\n",
    "        \"\"\"Updates the visibility of parameter widgets based on the selected distribution type.\"\"\"\n",
    "        # Hide all parameter widgets first\n",
    "        for param_widget in [std_dev_input, x0_input, x1_input, k_input, lamb_input, polarity_input, zeta_input, omega_input, alpha_input, height_input, value_input]:\n",
    "            param_widget.layout.display = \"none\"\n",
    "\n",
    "        # Show relevant widgets based on the selected distribution type\n",
    "        if distribution_type in distribution_params:\n",
    "            for param_name in distribution_params[distribution_type]:\n",
    "                if param_name in param_widgets and param_widgets[param_name] is not None:\n",
    "                    param_widgets[param_name].layout.display = \"\"\n",
    "\n",
    "\n",
    "    def plot_distribution(distribution_type, xmin, xmax, n, **params):\n",
    "        \"\"\"Plots the selected distribution function.\"\"\"\n",
    "        distributions = {\n",
    "            \"Gaussian\": gaussian,\n",
    "            \"Weibull\": weibull,\n",
    "            \"Skewed Gaussian\": skewed_gaussian,\n",
    "            \"Constant\": constant\n",
    "        }\n",
    "\n",
    "        if distribution_type not in distributions:\n",
    "            print(\"Invalid distribution type selected.\")\n",
    "            return\n",
    "\n",
    "        x = np.linspace(xmin, xmax, n+1)\n",
    "        y = []\n",
    "        arguments = []  # parameters for the user to use to produce this distribution\n",
    "\n",
    "        # Use the Output widget as the context for clearing and displaying the plot\n",
    "        with plot_output:\n",
    "            clear_output(wait=True)\n",
    "            x0 = params.get(\"x0\", (xmin + xmax) / 2)    # Default to (xmin+xmax)/2 if missing (shouldn't be)\n",
    "\n",
    "            # Handle Delta function separately for plotting\n",
    "            if distribution_type == \"Delta\":\n",
    "                height = params.get(\"height\", 1.0)\n",
    "                # For delta, we represent it as a single point at x0 with a stem plot\n",
    "                print(f\"\\t\\t['Delta', {', '.join(f'{i:g}' for i in arguments)}]\")     # Show user the command to define this error function\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=[x0], y=[height], mode=\"markers\", name=\"Relative frequency\", marker_color=HISTOGRAM_COLOUR))\n",
    "                fig.update_layout(xaxis_title=\"x\", yaxis_title=\"Relative frequency\", width=400, height=350, margin=dict(l=20, r=20, t=15, b=20))\n",
    "                apply_styles(fig, image=False)\n",
    "                fig.update_yaxes(showgrid=True, gridwidth=GRID_WIDTH, gridcolor=GRID_COLOUR)\n",
    "                fig.show()\n",
    "                arguments = [x0, height]\n",
    "            else:\n",
    "                dist_func = distributions[distribution_type]\n",
    "                if distribution_type == \"Weibull\":\n",
    "                    # Ensure k and lamb are present in params, use defaults if not\n",
    "                    k_val = params.get(\"k\", 5)\n",
    "                    lamb_val = params.get(\"lamb\", 3)\n",
    "                    polarity_val = params.get(\"polarity\", -1)\n",
    "                    y = [dist_func(xi, x0, lamb_val, k_val, polarity_val) for xi in x]\n",
    "                    arguments = [x0, lamb_val, k_val, polarity_val]\n",
    "                elif distribution_type == \"Constant\":\n",
    "                    # Ensure value is present in params, use default if not\n",
    "                    value_val = params.get(\"value\", 0.0)\n",
    "                    y = [dist_func(xi, value_val) for xi in x]\n",
    "                    arguments = [value_val]\n",
    "                elif distribution_type == \"Tophat\":\n",
    "                    # Ensure height is present in params, use default if not\n",
    "                    height_val = params.get(\"height\", 1.0)\n",
    "                    x1 = params.get('x1', 3.0)\n",
    "                    y = [dist_func(xi, x0, x1, height_val) for xi in x]\n",
    "                    arguments = [x0, x1, height_val]\n",
    "                elif distribution_type == \"Skewed Gaussian\":\n",
    "                    x0 = params.get(\"zeta\", (xmin + xmax) / 2)    # Default to (xmin+xmax)/2 if missing (shouldn't be)\n",
    "                    omega = params.get(\"omega\", 0.1)    # Omega, default 0.1\n",
    "                    alpha = params.get(\"alpha\", 1.0)  # Alpha, default 1\n",
    "                    # Limit the range of values to 4 standard deviations from the mean\n",
    "                    x = np.linspace(x0 - 4*omega, x0 + 4*omega, n+1)\n",
    "                    y = [dist_func(xi, x0, omega, alpha) for xi in x]\n",
    "                    arguments = [x0, omega, alpha]\n",
    "                else: # Gaussian\n",
    "                    std_dev = params.get(\"std_dev\", 0.1)\n",
    "                    # Limit the range of values to 4 standard deviations from the mean\n",
    "                    x = np.linspace(x0 - 4*std_dev, x0 + 4*std_dev, n+1)\n",
    "                    y = [dist_func(xi, x0, std_dev) for xi in x]\n",
    "                    arguments = [x0, std_dev]\n",
    "\n",
    "                # Show user the command to define this distribution (e.g. \"['Gaussian', 1, 0.05]\")\n",
    "                if distribution_type != \"Weibull\":\n",
    "                    print(f\"\\t\\t['{distribution_type.replace(' ', '')}', {', '.join(f'{i:g}' for i in arguments)}]\")\n",
    "                else:\n",
    "                    print(f\"\\t\\t['{distribution_type.replace(' ', '')}', {', '.join(f'{i:g}' for i in arguments[0:-1])}, {arguments[-1]}]\")\n",
    "                # Normalise the y-values to have 1 as the maximum\n",
    "                y = [yi/max(y) for yi in y]\n",
    "                # Plot the distribution\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=\"Relative frequency\", marker_color=HISTOGRAM_COLOUR))\n",
    "                fig.update_layout(xaxis_title=\"x\", yaxis_title=\"Relative frequency\", width=400, height=350, margin=dict(l=20, r=20, t=15, b=20))\n",
    "                fig.update_xaxes(range=[xmin, xmax])\n",
    "                fig.update_yaxes(range=[0, math.ceil(max(y)*10)/10+0.1])\n",
    "                apply_styles(fig, image=False)\n",
    "                fig.show()\n",
    "\n",
    "\n",
    "    # Link the widgets to the interactive wrapper function\n",
    "    def interactive_plot_wrapper(distribution_type, xmin, xmax, n, **param_values):\n",
    "        \"\"\"Wrapper function to handle interactive updates and call plot_distribution.\"\"\"\n",
    "        update_param_visibility(distribution_type)\n",
    "\n",
    "        # Prepare parameters for plot_distribution based on the selected type\n",
    "        params_to_pass = {}\n",
    "        if distribution_type in distribution_params:\n",
    "            for param_name in distribution_params[distribution_type]:\n",
    "                # Check if the parameter is one of the specific distribution parameters and exists in param_values\n",
    "                if param_name in [\"mean\", \"std_dev\", \"x0\", \"x1\", \"k\", \"lamb\", \"polarity\", \"zeta\", \"omega\", \"alpha\", \"height\", \"value\"] and param_name in param_values:\n",
    "                    params_to_pass[param_name] = param_values[param_name]\n",
    "                # x_min and x_max for Tophat will be taken from the main xmin and xmax inputs in plot_distribution\n",
    "\n",
    "        plot_distribution(distribution_type, xmin, xmax, n, **params_to_pass)\n",
    "\n",
    "\n",
    "    interactive_plot = widgets.interactive(\n",
    "        interactive_plot_wrapper,\n",
    "        distribution_type=distribution_dropdown,\n",
    "        xmin=xmin_input,\n",
    "        xmax=xmax_input,\n",
    "        n=n_input,\n",
    "        # Pass all potential parameter widgets to interactive, their values will be available in param_values\n",
    "        std_dev=param_widgets[\"std_dev\"],\n",
    "        x0=param_widgets[\"x0\"],\n",
    "        x1=param_widgets[\"x1\"],\n",
    "        k=param_widgets[\"k\"],\n",
    "        lamb=param_widgets[\"lamb\"],\n",
    "        polarity=param_widgets[\"polarity\"],\n",
    "        zeta=param_widgets[\"zeta\"],\n",
    "        omega=param_widgets[\"omega\"],\n",
    "        alpha=param_widgets[\"alpha\"],\n",
    "        height=param_widgets[\"height\"],\n",
    "        value=param_widgets[\"value\"]\n",
    "    )\n",
    "\n",
    "    # Group the parameter specific widgets in two columns\n",
    "    param_widget_list = [x0_input, x1_input, zeta_input, std_dev_input, alpha_input, omega_input, k_input, lamb_input, height_input, polarity_input, value_input]\n",
    "    # Split the widgets into two columns (adjust as needed based on the number of parameters)\n",
    "    col1_widgets = param_widget_list[::2] # Every other widget starting from the first\n",
    "    col2_widgets = param_widget_list[1::2] # Every other widget starting from the second\n",
    "\n",
    "    # Group the main control widgets\n",
    "    control_widgets = widgets.VBox([\n",
    "        distribution_dropdown,\n",
    "        widgets.HBox([widgets.VBox(col1_widgets), widgets.VBox(col2_widgets)])\n",
    "    ])\n",
    "    # Group the plot control widgets\n",
    "    plot_widgets = widgets.VBox([\n",
    "        plot_header,\n",
    "        widgets.HBox([xmin_input, xmax_input, n_input], layout=widgets.Layout(width='95%')), # Arrange xmin and xmax horizontally\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Combine control widgets, parameter widgets, and the interactive plot output\n",
    "    interactive_display = widgets.VBox([\n",
    "        # widgets.Label(\"Interactive Distribution Modifier\"), # Add a title\n",
    "        control_widgets,\n",
    "        plot_output, # Display the dedicated Output widget for the plot\n",
    "        plot_widgets,\n",
    "    ])\n",
    "\n",
    "    # Display the organized interactive plot\n",
    "    display(interactive_display)\n",
    "\n",
    "    # Initially update parameter visibility based on the default dropdown value\n",
    "    update_param_visibility(distribution_dropdown.value)\n",
    "\n",
    "    # Trigger an initial plot display\n",
    "    interactive_plot_wrapper(distribution_dropdown.value, xmin_input.value, xmax_input.value, n_input.value,\n",
    "                            std_dev=std_dev_input.value, x0=x0_input.value, x1=x1_input.value, k=k_input.value,\n",
    "                            lamb=lamb_input.value, polarity=polarity_input.value, alpha=alpha_input.value,\n",
    "                            zeta=zeta_input.value, omega=omega_input.value, height=height_input.value, value=value_input.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "31PPkxWNSn-C"
   },
   "outputs": [],
   "source": [
    "# Define export function\n",
    "def export_to_excel(data, filename, used_inputs=None, dist_list=None):\n",
    "    # Bins go: var numBins = (int)((1 - request.MinP) * 2 / request.DeltaP + 0.5);\n",
    "    bin_min = used_inputs.ResultOptions.PMin\n",
    "    bin_width = used_inputs.ResultOptions.PDelta\n",
    "    # Process YearlyPValue data\n",
    "    pvalues_data = {}\n",
    "    unique_p_values = sorted(list(set([item.P for item in data.YearlyPValue])))\n",
    "    unique_years = sorted(list(set([item.Year for item in data.YearlyPValue])))\n",
    "\n",
    "    pvalues_data[\"PValue\"] = unique_p_values\n",
    "    for year in unique_years:\n",
    "        year_data = [item.P50Deviation for item in data.YearlyPValue if item.Year == year]\n",
    "        pvalues_data[f\"Year {year}\"] = year_data\n",
    "\n",
    "    pvalues_df = pd.DataFrame(pvalues_data)\n",
    "\n",
    "    # Process YearlyP50Deviation data\n",
    "    yearly_p50_data = {}\n",
    "    p50_values = [item.Value for item in data.YearlyP50Deviation]\n",
    "    yearly_p50_data[\"P50Deviation\"] = [\"P50\"]\n",
    "    for year, p50 in zip(unique_years, p50_values):\n",
    "        yearly_p50_data[f\"Year {year}\"] = p50\n",
    "\n",
    "    p50_df = pd.DataFrame(yearly_p50_data)\n",
    "\n",
    "    # Process YearlyHistogram data\n",
    "    histograms_data = {}\n",
    "    unique_years_hist = sorted(list(set([item.Year for item in data.YearlyHistogram])))\n",
    "\n",
    "    # Define histogram bins based on bin_min and bin_width\n",
    "    num_bins = len(data.YearlyHistogram[0].bins)\n",
    "    bin_max = 1 + (1-bin_min) - bin_width\n",
    "    bin_labels = [round(i, 2) for i in np.linspace(bin_min, bin_max, num_bins)]\n",
    "\n",
    "    # # Add custom labels for the ends as requested, assuming they correspond to the first and last bins\n",
    "    # bin_labels[0] = f'<{bin_min:.2f}'     # The first bin is less than bin_min\n",
    "    # bin_labels[-1] = f'>{bin_max:.2f}'    # The last bin is greater than bin_max+bin_width\n",
    "\n",
    "    histograms_data[\"Bin\"] = bin_labels\n",
    "\n",
    "    for year in unique_years_hist:\n",
    "        year_histogram = [list(item.bins) for item in data.YearlyHistogram if item.Year == year][0]\n",
    "        histograms_data[f\"Year {year}\"] = year_histogram\n",
    "\n",
    "    histograms_df = pd.DataFrame(histograms_data)\n",
    "\n",
    "    # Add a dataframe with the used inputs, if supplied\n",
    "    if used_inputs is not None:\n",
    "        # Convert used_inputs to a structured DataFrame for CSV export\n",
    "\n",
    "        # Extract all the key parameters from used_inputs\n",
    "        data = {\n",
    "            \"Parameter\": [\n",
    "                \"Module Length (m)\", \"Module Width (m)\", \"Module Power (W)\", \"Module Efficiency Temp Coeff\",\n",
    "                \"Module Height Above Ground (m)\", \"Bifaciality\", \"Cell to Cell Mismatch\",\n",
    "                \"Number of Inverters\", \"Strings per Inverter\", \"Modules per String\", \"Row Pitch (m)\",\n",
    "                \"Module Azimuth (degrees)\", \"Module Tilt (degrees)\",\n",
    "                \"Inverter Efficiency\", \"Inverter to Inverter Mismatch\", \"String to String Mismatch\",\n",
    "                \"Module to Module Mismatch\", \"Inverter Wiring Loss\", \"String Wiring Loss\", \"Max Power Tracking Loss\",\n",
    "                \"Beam Multiplier Front\", \"Isotropic Multiplier Front\", \"Beam Multiplier Rear\", \"Isotropic Multiplier Rear\",\n",
    "                \"Fallback Albedo\", \"Fallback Soiling Front\", \"Fallback Soiling Rear\", \"Fallback Spectral Correction\",\n",
    "                \"Extra Irradiance\", \"Thermal Uc\", \"Thermal Uv\", \"Thermal Alpha\",\n",
    "                \"Annual Degradation Rate\", \"DC Health\", \"Availability\", \"Curtailment\", \"Yield Modifier\", \"Undulating Ground\",\n",
    "                \"Number of Simulations\", \"Number of Years\", \"P Min\", \"P Delta\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                used_inputs.Module.LengthInM, used_inputs.Module.WidthInM, used_inputs.Module.PowerRatingAtSTCInW,\n",
    "                used_inputs.Module.ModuleEfficiencyTemperatureCoefficient, used_inputs.Module.HeightAboveGroundInM,\n",
    "                used_inputs.Module.Bifaciality, used_inputs.Module.CellToCellMismatch,\n",
    "                used_inputs.System.NumberOfInverters, used_inputs.System.StringsPerInverter, used_inputs.System.ModulesPerString,\n",
    "                used_inputs.System.RowPitchInM, used_inputs.System.ModuleAzimuthInDegrees, used_inputs.System.FallbackModuleTiltInDegrees,\n",
    "                used_inputs.Electrical.InverterEfficiency, used_inputs.Electrical.InverterToInverterMismatch,\n",
    "                used_inputs.Electrical.StringToStringMismatch, used_inputs.Electrical.ModuleToModuleMismatch,\n",
    "                used_inputs.Electrical.InverterWiringLoss, used_inputs.Electrical.StringWiringLoss, used_inputs.Electrical.MaxPowerTrackingLoss,\n",
    "                used_inputs.Optical.BeamMultiplierFront, used_inputs.Optical.IsotropicMultiplierFront,\n",
    "                used_inputs.Optical.BeamMultiplierRear, used_inputs.Optical.IsotropicMultiplierRear,\n",
    "                used_inputs.Optical.FallbackAlbedo, used_inputs.Optical.FallbackSoilingFront, used_inputs.Optical.FallbackSoilingRear,\n",
    "                used_inputs.Optical.FallbackSpectralCorrection, used_inputs.Optical.ExtraIrradiance,\n",
    "                used_inputs.Thermal.Uc, used_inputs.Thermal.Uv, used_inputs.Thermal.Alpha,\n",
    "                used_inputs.Operation.AnnualDegradationRate, used_inputs.Operation.DCHealth, used_inputs.Operation.Availability,\n",
    "                used_inputs.Operation.Curtailment, used_inputs.Operation.YieldModifier, used_inputs.Operation.UndulatingGround,\n",
    "                used_inputs.SimulationOptions.NumberOfSimulations, used_inputs.SimulationOptions.NumberOfYears,\n",
    "                used_inputs.ResultOptions.PMin, used_inputs.ResultOptions.PDelta\n",
    "            ],\n",
    "            \"Units\": [\n",
    "                \"m\", \"m\", \"W\", \"/degC\", \"m\", \"-\", \"-\",\n",
    "                \"-\", \"-\", \"-\", \"m\", \"degrees\", \"degrees\",\n",
    "                \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\",\n",
    "                \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\",\n",
    "                \"W/m2K\", \"W/m3sK\", \"-\",\n",
    "                \"/year\", \"-\", \"-\", \"-\", \"-\", \"-\",\n",
    "                \"-\", \"years\", \"-\", \"-\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Add P Values as separate rows\n",
    "        for p_val in used_inputs.ResultOptions.PValues:\n",
    "            data[\"Parameter\"].append(f'P Value {p_val}')\n",
    "            data[\"Value\"].append(p_val)\n",
    "            data[\"Units\"].append('-')\n",
    "\n",
    "        used_inputs_df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to Excel\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        filename = filename[:-5]\n",
    "    with pd.ExcelWriter(f\"{filename}.xlsx\") as writer:\n",
    "        pvalues_df.to_excel(writer, sheet_name=\"PValues\", index=False)\n",
    "        p50_df.to_excel(writer, sheet_name=\"P50Deviations\", index=False)\n",
    "        histograms_df.to_excel(writer, sheet_name=\"Histograms\", index=False)\n",
    "        if used_inputs is not None:\n",
    "            used_inputs_df.to_excel(writer, sheet_name=\"UsedInputs\", index=False)\n",
    "        # Add a dataframe with the distribution list, if supplied\n",
    "        if dist_list is not None:\n",
    "            dist_df = pd.DataFrame(parse_distribution_list(dist_list)).T    # .T for transpose, to have each input as a row, instead of column\n",
    "            dist_df.to_excel(writer, sheet_name=\"Distributions\", index=True)    # index=True, as the input names are in the index after .T\n",
    "\n",
    "    print(f\"Summary data saved to '{filename}.xlsx'\")\n",
    "\n",
    "# Define notebook export function\n",
    "def export_notebook(current_notebook, result_notebook):\n",
    "    try:\n",
    "        with open(current_notebook, 'r', encoding='utf-8') as src:\n",
    "            notebook_content = src.read()\n",
    "        \n",
    "        with open(result_notebook, 'w', encoding='utf-8') as dst:\n",
    "            dst.write(notebook_content)\n",
    "        \n",
    "        print(f\"Notebook copy created: {result_notebook}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not find {current_notebook} to copy. You may need to manually save your notebook.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating notebook copy: {e}\")\n",
    "\n",
    "# Parse distribution list to extract uncertainties\n",
    "def parse_distribution_list(distribution_list):\n",
    "    uncertainties = {}\n",
    "    for dist in distribution_list:\n",
    "        # Get input name\n",
    "        dist_str = str(dist)\n",
    "        if hasattr(dist, \"Input\") and dist.Input != 0:\n",
    "            input_name = re.search(r\"(?<=Input: )\\S+\", dist_str).group()\n",
    "        else:\n",
    "            input_name = 'GHI'  # Default for first distribution\n",
    "        \n",
    "        # Initialize distribution parameters\n",
    "        sim_to_sim = None\n",
    "        yr_to_yr = None\n",
    "        step_to_step = None\n",
    "        \n",
    "        # Parse SimToSim Distribution\n",
    "        if hasattr(dist, \"SimToSimDistribution\"):\n",
    "            sim_dist = dist.SimToSimDistribution\n",
    "            enum_type = sim_dist.DESCRIPTOR.fields_by_name[\"Type\"].enum_type\n",
    "            dist_name = enum_type.values[sim_dist.Type].name\n",
    "            match dist_name:\n",
    "                case \"Gaussian\":\n",
    "                    x0 = sim_dist.Gaussian.x0\n",
    "                    sigma = sim_dist.Gaussian.sigma\n",
    "                    sim_to_sim = [\"Gaussian\", x0, sigma]\n",
    "                case \"SkewedGaussian\":\n",
    "                    alpha = sim_dist.SkewedGaussian.alpha\n",
    "                    zeta = sim_dist.SkewedGaussian.zeta\n",
    "                    omega = sim_dist.SkewedGaussian.omega\n",
    "                    sim_to_sim = [\"SkewedGaussian\", zeta, omega, alpha]\n",
    "                case \"Weibull\":\n",
    "                    x0 = sim_dist.Weibull.x0\n",
    "                    lambda_val = getattr(sim_dist.Weibull, \"lambda\")\n",
    "                    k = sim_dist.Weibull.k\n",
    "                    positive_polarity = sim_dist.Weibull.hasPositivePolarity\n",
    "                    sim_to_sim = [\"Weibull\", x0, lambda_val, k, positive_polarity]\n",
    "                case \"Constant\":\n",
    "                    # No distribution applied\n",
    "                    pass\n",
    "                case _:\n",
    "                    raise ValueError(f\"Unknown distribution type: {dist_name}\")\n",
    "        \n",
    "        # Parse YrToYr Distribution\n",
    "        if hasattr(dist, \"YrToYrDistribution\"):\n",
    "            year_dist = dist.YrToYrDistribution\n",
    "            enum_type = year_dist.DESCRIPTOR.fields_by_name[\"Type\"].enum_type\n",
    "            dist_name = enum_type.values[year_dist.Type].name\n",
    "            match dist_name:\n",
    "                case \"Gaussian\":\n",
    "                    x0 = year_dist.Gaussian.x0\n",
    "                    sigma = year_dist.Gaussian.sigma\n",
    "                    yr_to_yr = [\"Gaussian\", x0, sigma]\n",
    "                case \"SkewedGaussian\":\n",
    "                    alpha = year_dist.SkewedGaussian.alpha\n",
    "                    zeta = year_dist.SkewedGaussian.zeta\n",
    "                    omega = year_dist.SkewedGaussian.omega\n",
    "                    yr_to_yr = [\"SkewedGaussian\", zeta, omega, alpha]\n",
    "                case \"Weibull\":\n",
    "                    x0 = year_dist.Weibull.x0\n",
    "                    lambda_val = getattr(year_dist.Weibull, \"lambda\")\n",
    "                    k = year_dist.Weibull.k\n",
    "                    positive_polarity = year_dist.Weibull.hasPositivePolarity\n",
    "                    yr_to_yr = [\"Weibull\", lambda_val, k, positive_polarity]\n",
    "                case \"Constant\":\n",
    "                    # No distribution applied\n",
    "                    pass\n",
    "                case _:\n",
    "                    raise ValueError(f\"Unknown distribution type: {dist_name}\")\n",
    "                \n",
    "        # Parse StepToStep Distribution\n",
    "        if hasattr(dist, \"StepToStepDistribution\"):\n",
    "            step_dist = dist.StepToStepDistribution\n",
    "            enum_type = step_dist.DESCRIPTOR.fields_by_name[\"Type\"].enum_type\n",
    "            dist_name = enum_type.values[step_dist.Type].name\n",
    "            match dist_name:\n",
    "                case \"Gaussian\":\n",
    "                    x0 = step_dist.Gaussian.x0\n",
    "                    sigma = step_dist.Gaussian.sigma\n",
    "                    step_to_step = [\"Gaussian\", x0, sigma]\n",
    "                case \"SkewedGaussian\":\n",
    "                    alpha = step_dist.SkewedGaussian.alpha\n",
    "                    zeta = step_dist.SkewedGaussian.zeta\n",
    "                    omega = step_dist.SkewedGaussian.omega\n",
    "                    step_to_step = [\"SkewedGaussian\", zeta, omega, alpha]\n",
    "                case \"Weibull\":\n",
    "                    x0 = step_dist.Weibull.x0\n",
    "                    lambda_val = getattr(step_dist.Weibull, \"lambda\")\n",
    "                    k = step_dist.Weibull.k\n",
    "                    positive_polarity = step_dist.Weibull.hasPositivePolarity\n",
    "                    step_to_step = [\"Weibull\", lambda_val, k, positive_polarity]\n",
    "                case \"Constant\":\n",
    "                    # No distribution applied\n",
    "                    pass\n",
    "                case _:\n",
    "                    raise ValueError(f\"Unknown distribution type: {dist_name}\")\n",
    "        \n",
    "        uncertainties[input_name] = {\n",
    "            \"sim_to_sim\": sim_to_sim,\n",
    "            \"yr_to_yr\": yr_to_yr,\n",
    "            \"step_to_step\": step_to_step\n",
    "        }\n",
    "    return uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvlib comparison functions - for the pvlib comparison notebook\n",
    "\n",
    "# Helper function to generate random values from distribution specifications\n",
    "def generate_random_value(dist_spec):\n",
    "    \"\"\"Generate random value from distribution specification\"\"\"\n",
    "    if dist_spec is None:\n",
    "        return 1.0  # No variation\n",
    "    \n",
    "    dist_type = dist_spec[0]\n",
    "    if dist_type == \"Gaussian\":\n",
    "        mean, sigma = dist_spec[1], dist_spec[2]\n",
    "        return np.random.normal(mean, sigma)\n",
    "    elif dist_type == \"SkewedGaussian\":\n",
    "        zeta, omega, alpha = dist_spec[1], dist_spec[2], dist_spec[3]\n",
    "        return skewnorm.rvs(a=alpha, loc=zeta, scale=omega)\n",
    "    elif dist_type == \"Weibull\":\n",
    "        x0, lambda_param, k = dist_spec[1:4]\n",
    "        positive_polarity = dist_spec[4] if len(dist_spec) > 4 else False\n",
    "        p = 1 if positive_polarity else -1\n",
    "        u = np.random.uniform(0, 1)\n",
    "        # Inverse CDF calculation\n",
    "        return x0 + lambda_param * p * (-np.log(1 - u)) ** (1 / k)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "# Helper function to pass distribution specs for generating random values\n",
    "def random_val(distributions_dict, input, variation_type):\n",
    "    \"\"\"Get random value for a specific input and variation type\"\"\"\n",
    "    dist_spec = distributions_dict.get(input, {}).get(variation_type, None)\n",
    "    if dist_spec is None:\n",
    "        if variation_type not in [\"sim_to_sim\", \"yr_to_yr\", \"step_to_step\"]:\n",
    "            raise ValueError(f\"Invalid variation type: {variation_type}\")\n",
    "        return 1.0  # No variation specified\n",
    "    \n",
    "    # Retrieve relevant bounds for the input\n",
    "    enum_value = getattr(DistributionInput, input, None)\n",
    "    if enum_value is None:\n",
    "        raise ValueError(f\"Invalid input name: {input}\")\n",
    "    lower_bound, upper_bound = DISTRIBUTION_LIMITS[enum_value]\n",
    "\n",
    "    # Generate values until one falls within the specified bounds, safely\n",
    "    attempts = 0\n",
    "    while attempts < 1000:\n",
    "        value = generate_random_value(dist_spec)\n",
    "        if lower_bound <= value <= upper_bound:\n",
    "            return value\n",
    "        attempts += 1\n",
    "    raise ValueError(f\"Failed to generate valid value for {input} and variation type {variation_type} after {attempts} attempts\")\n",
    "\n",
    "\n",
    "# Convert pvlib data into an UncertaintySummary to match the SunSolve P90 data\n",
    "def create_pvlib_summary(annual_yield_list, num_years, yield_bins):\n",
    "\n",
    "    bin_min, bin_width = yield_bins[0], yield_bins[1]-yield_bins[0]\n",
    "    # aligning with structure of pvl_p90_client.grpcclientuncertaintyMessages_pb2.UncertaintySummary\n",
    "    class PVLibSummary:\n",
    "        def __init__(self):\n",
    "            self.YearlyHistogram = []\n",
    "            self.YearlyP50Deviation = []\n",
    "    class YearHistogram:\n",
    "        def __init__(self):\n",
    "            self.bins = []\n",
    "            self.Year = None\n",
    "    class YearlyP50Deviation:\n",
    "        def __init__(self, year=None, value=None):\n",
    "            self.Year = year\n",
    "            self.Value = value\n",
    "\n",
    "    pvlib_summary = PVLibSummary()\n",
    "\n",
    "    for i in range(num_years):\n",
    "        year_values = YearHistogram()\n",
    "        year_values.Year = i+1\n",
    "        yields = annual_yield_list[:, i]\n",
    "        p50 = np.median(yields)\n",
    "        if i == 0:\n",
    "            year_one_p50 = p50\n",
    "        relative_yields = yields / (p50 if p50 != 0 else 1)   # Avoid division by zero\n",
    "        year_values.bins = [sum(y < bin_min+bin_width for y in relative_yields)]\n",
    "        year_values.bins += [sum(bin < y < bin+bin_width for y in relative_yields) for bin in yield_bins[1:-1]]\n",
    "        year_values.bins += [sum(y > yield_bins[-1] for y in relative_yields) ]\n",
    "        pvlib_summary.YearlyHistogram.append(year_values)\n",
    "        pvlib_summary.YearlyP50Deviation.append(YearlyP50Deviation(year=i, value=p50/(year_one_p50 if year_one_p50 != 0 else 1)))\n",
    "    \n",
    "    return pvlib_summary"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPoBjqmxwiFXgYL1FHEufAA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
